{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81e7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f98c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/tqdm_joblib/__init__.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from tqdm import tqdm\n",
    "from general_utils.constants import spectral_bands, indices\n",
    "\n",
    "\n",
    "class TimeSeriesAggregator:\n",
    "    DEFAULT_FEATURES = spectral_bands + indices\n",
    "\n",
    "    def __init__(self, features=None, window=120, step=90, n_jobs=-1):\n",
    "        self.features = features or self.DEFAULT_FEATURES\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_trend(x, y):\n",
    "        if len(x) < 2:\n",
    "            return 0\n",
    "        xm, ym = np.mean(x), np.mean(y)\n",
    "        denom = np.sum((x - xm) ** 2)\n",
    "        return 0 if denom == 0 else np.sum((x - xm) * (y - ym)) / denom\n",
    "\n",
    "    def process_id(self, id_, group):\n",
    "        group = group.sort_values(\"time\")\n",
    "        species = group[\"species\"].iloc[0]\n",
    "        times = group[\"time_num\"].values\n",
    "        feats = []\n",
    "        start = 0\n",
    "\n",
    "        while start < times.max():\n",
    "            end = start + self.window\n",
    "            win = group[(group[\"time_num\"] >= start) & (group[\"time_num\"] < end)]\n",
    "            if len(win) < 2:\n",
    "                start += self.step\n",
    "                continue\n",
    "\n",
    "            f = {\"id\": id_, \"species\": species}\n",
    "            for col in self.features:\n",
    "                vals = win[col].values\n",
    "                t = win[\"time_num\"].values\n",
    "                f[f\"{col}_mean\"] = vals.mean()\n",
    "                f[f\"{col}_std\"] = vals.std()\n",
    "                f[f\"{col}_min\"] = vals.min()\n",
    "                f[f\"{col}_max\"] = vals.max()\n",
    "                f[f\"{col}_trend\"] = self.calc_trend(t, vals)\n",
    "            feats.append(f)\n",
    "            start += self.step\n",
    "        return feats\n",
    "\n",
    "    def aggregate_time_series(self, df):\n",
    "        df = df.copy()\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        df[\"time_num\"] = (df[\"time\"] - df[\"time\"].min()).dt.days\n",
    "        groups = list(df.groupby(\"id\"))\n",
    "\n",
    "        with tqdm_joblib(\n",
    "            tqdm(desc=\"Processing IDs\", total=len(groups))\n",
    "        ) as progress_bar:\n",
    "            res = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(self.process_id)(i, g) for i, g in groups\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame([f for sub in res for f in sub])\n",
    "\n",
    "    def aggregate_to_single_row_keep_windows(self, df_windows):\n",
    "        df_windows = df_windows.copy()\n",
    "        df_windows[\"window_idx\"] = df_windows.groupby(\"id\").cumcount()\n",
    "        df_pivot = df_windows.pivot_table(index=[\"id\", \"species\"], columns=\"window_idx\")\n",
    "        df_pivot.columns = [\n",
    "            f\"{col[0]}_w{col[1]}\" if isinstance(col, tuple) else col\n",
    "            for col in df_pivot.columns\n",
    "        ]\n",
    "        return df_pivot.reset_index()\n",
    "\n",
    "    def run(self, df):\n",
    "        \"\"\"Shortcut: vollständiger Lauf.\"\"\"\n",
    "        df_win = self.aggregate_time_series(df)\n",
    "        return self.aggregate_to_single_row_keep_windows(df_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c248a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_builder = TimeSeriesAggregator(window=120, step=90)\n",
    "# train_df = ts_builder.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f5e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_transform(self, path):\n",
    "        \"\"\"load data and perform basic aggregation and sorting\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "        except pd.errors.ParserError:\n",
    "            raise ValueError(f\"File {path} could not be read as CSV.\")\n",
    "\n",
    "        if not {\"time\", \"id\"}.issubset(df.columns):\n",
    "            raise ValueError(\"CSV must contain 'time' and 'id' columns.\")\n",
    "\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "        if df[\"time\"].isna().any():\n",
    "            raise ValueError(\n",
    "                \"Some values in 'time' could not be converted to datetime.\"\n",
    "            )\n",
    "\n",
    "        agg_dict = {\n",
    "            col: \"mean\" if pd.api.types.is_numeric_dtype(dtype) else \"first\"\n",
    "            for col, dtype in df.dtypes.items()\n",
    "            if col not in [\"time\", \"id\"]\n",
    "        }\n",
    "\n",
    "        df = df.groupby([\"time\", \"id\"], as_index=False).agg(agg_dict)\n",
    "        df = df.sort_values([\"id\", \"time\"]).reset_index(drop=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edef898",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../../../data/processed/trainset.csv\"\n",
    "TEST_PATH = \"../../../data/processed/testset.csv\"\n",
    "VAL_PATH = \"../../../data/processed/valset.csv\"\n",
    "\n",
    "\n",
    "def load_data(*paths: Path):\n",
    "    \"\"\"\n",
    "    Loads and transforms data from one or more file paths.\n",
    "\n",
    "    Returns a DataFrame if one path is given, otherwise a list of DataFrames.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader()\n",
    "    dataframes = []\n",
    "\n",
    "    for path in paths:\n",
    "        df = dataloader.load_transform(path)\n",
    "        print(f\"Data import and transformation finished for: {path}\")\n",
    "        dataframes.append(df)\n",
    "\n",
    "    if len(dataframes) == 1:\n",
    "        return dataframes[0]\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def define_models():\n",
    "    return [\n",
    "        (RandomForestClassifier, {\"n_estimators\": 5, \"max_depth\": 2}),\n",
    "        # (lgb.LGBMClassifier, {\"n_estimators\": 100, \"learning_rate\": 0.1}),\n",
    "        # (xgb.XGBClassifier, {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 6}),\n",
    "    ]\n",
    "\n",
    "\n",
    "def train_models(train_df, test_df, models, target_col=\"species\"):\n",
    "    pipeline = GenericPipeline(target_col=target_col)\n",
    "    pipeline.run(train_df, test_df, models)\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29772d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data import and transformation finished for: ../../../data/processed/trainset.csv\n",
      "Data import and transformation finished for: ../../../data/processed/testset.csv\n",
      "Data import and transformation finished for: ../../../data/processed/valset.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "disturbance_year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "doy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b8a",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "b12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "species",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9b83a569-96e2-43ba-bdd9-00492d008588",
       "rows": [
        [
         "0",
         "2017-01-13 00:00:00",
         "4",
         "0.0",
         "13.0",
         "176.0",
         "229.0",
         "205.0",
         "405.0",
         "1192.0",
         "1308.0",
         "1316.0",
         "1529.0",
         "396.0",
         "180.0",
         "Scots_pine"
        ],
        [
         "1",
         "2017-01-27 00:00:00",
         "4",
         "0.0",
         "27.0",
         "382.0",
         "429.0",
         "205.0",
         "405.0",
         "1192.0",
         "1308.0",
         "1316.0",
         "1529.0",
         "396.0",
         "180.0",
         "Scots_pine"
        ],
        [
         "2",
         "2017-02-19 00:00:00",
         "4",
         "0.0",
         "50.0",
         "240.0",
         "382.0",
         "259.0",
         "544.0",
         "1387.0",
         "1609.0",
         "1825.0",
         "1762.0",
         "732.0",
         "338.0",
         "Scots_pine"
        ],
        [
         "3",
         "2017-03-27 00:00:00",
         "4",
         "0.0",
         "86.0",
         "244.0",
         "375.0",
         "287.0",
         "579.0",
         "1536.0",
         "1776.0",
         "1878.0",
         "1884.0",
         "1013.0",
         "488.0",
         "Scots_pine"
        ],
        [
         "4",
         "2017-04-27 00:00:00",
         "4",
         "0.0",
         "117.0",
         "215.0",
         "327.0",
         "274.0",
         "548.0",
         "1499.0",
         "1803.0",
         "1766.0",
         "1957.0",
         "1003.0",
         "540.0",
         "Scots_pine"
        ],
        [
         "5",
         "2017-05-28 00:00:00",
         "4",
         "0.0",
         "148.0",
         "218.0",
         "347.0",
         "290.0",
         "547.0",
         "1409.0",
         "1756.0",
         "1803.0",
         "2064.0",
         "1155.0",
         "497.0",
         "Scots_pine"
        ],
        [
         "6",
         "2017-07-28 00:00:00",
         "4",
         "0.0",
         "209.0",
         "161.0",
         "311.0",
         "212.0",
         "517.0",
         "1672.0",
         "1885.0",
         "2072.0",
         "2288.0",
         "916.0",
         "412.0",
         "Scots_pine"
        ],
        [
         "7",
         "2017-08-16 00:00:00",
         "4",
         "0.0",
         "228.0",
         "233.0",
         "340.0",
         "254.0",
         "561.0",
         "1595.0",
         "1949.0",
         "2261.0",
         "2312.0",
         "947.0",
         "394.0",
         "Scots_pine"
        ],
        [
         "8",
         "2017-08-30 00:00:00",
         "4",
         "0.0",
         "242.0",
         "496.0",
         "581.0",
         "498.0",
         "761.0",
         "1704.0",
         "2106.0",
         "2254.0",
         "2315.0",
         "1055.0",
         "447.0",
         "Scots_pine"
        ],
        [
         "9",
         "2018-02-06 00:00:00",
         "4",
         "0.0",
         "37.0",
         "485.0",
         "541.5",
         "113.0",
         "740.0",
         "1295.0",
         "1527.0",
         "1784.0",
         "1689.0",
         "437.0",
         "200.0",
         "Scots_pine"
        ],
        [
         "10",
         "2018-02-08 00:00:00",
         "4",
         "0.0",
         "39.0",
         "474.0",
         "502.0",
         "464.0",
         "719.0",
         "1540.0",
         "1747.0",
         "1946.0",
         "1938.0",
         "678.0",
         "301.0",
         "Scots_pine"
        ],
        [
         "11",
         "2018-02-11 00:00:00",
         "4",
         "0.0",
         "42.0",
         "193.0",
         "271.0",
         "190.0",
         "455.0",
         "1393.0",
         "1659.0",
         "1890.0",
         "1986.0",
         "587.0",
         "264.0",
         "Scots_pine"
        ],
        [
         "12",
         "2018-02-25 00:00:00",
         "4",
         "0.0",
         "56.0",
         "323.0",
         "392.25",
         "342.5",
         "571.0",
         "1765.0",
         "1935.0",
         "1960.0",
         "2113.0",
         "859.0",
         "274.75",
         "Scots_pine"
        ],
        [
         "13",
         "2018-03-03 00:00:00",
         "4",
         "0.0",
         "62.0",
         "453.0",
         "513.5",
         "495.0",
         "687.0",
         "1698.0",
         "1954.5",
         "2267.5",
         "2264.5",
         "865.0",
         "285.5",
         "Scots_pine"
        ],
        [
         "14",
         "2018-03-09 00:00:00",
         "4",
         "0.0",
         "68.0",
         "313.0",
         "457.0",
         "338.0",
         "577.0",
         "1534.0",
         "1827.0",
         "1951.0",
         "1985.0",
         "871.0",
         "428.0",
         "Scots_pine"
        ],
        [
         "15",
         "2018-03-17 00:00:00",
         "4",
         "0.0",
         "76.0",
         "440.0",
         "554.0",
         "465.0",
         "780.0",
         "1756.0",
         "1809.0",
         "1940.0",
         "2150.0",
         "744.0",
         "422.0",
         "Scots_pine"
        ],
        [
         "16",
         "2018-04-03 00:00:00",
         "4",
         "0.0",
         "93.0",
         "217.0",
         "326.0",
         "233.0",
         "583.0",
         "1447.0",
         "1809.0",
         "1851.0",
         "2047.0",
         "843.0",
         "359.0",
         "Scots_pine"
        ],
        [
         "17",
         "2018-04-07 00:00:00",
         "4",
         "0.0",
         "97.0",
         "386.0",
         "483.0",
         "403.0",
         "619.0",
         "1401.0",
         "1652.0",
         "1955.0",
         "1921.0",
         "1028.0",
         "460.0",
         "Scots_pine"
        ],
        [
         "18",
         "2018-04-21 00:00:00",
         "4",
         "0.0",
         "111.0",
         "322.0",
         "449.0",
         "365.0",
         "662.0",
         "1654.0",
         "1860.0",
         "2024.0",
         "2145.0",
         "1109.0",
         "554.0",
         "Scots_pine"
        ],
        [
         "19",
         "2018-04-23 00:00:00",
         "4",
         "0.0",
         "113.0",
         "242.0",
         "387.0",
         "285.0",
         "524.0",
         "1389.0",
         "1700.0",
         "1952.0",
         "1984.0",
         "959.0",
         "463.0",
         "Scots_pine"
        ],
        [
         "20",
         "2018-05-04 00:00:00",
         "4",
         "0.0",
         "124.0",
         "197.0",
         "316.0",
         "263.0",
         "577.0",
         "1543.0",
         "1897.0",
         "2015.0",
         "2186.0",
         "1041.0",
         "496.0",
         "Scots_pine"
        ],
        [
         "21",
         "2018-05-07 00:00:00",
         "4",
         "0.0",
         "127.0",
         "321.0",
         "470.0",
         "365.0",
         "640.0",
         "1533.0",
         "1887.0",
         "2029.0",
         "2104.0",
         "1074.0",
         "561.0",
         "Scots_pine"
        ],
        [
         "22",
         "2018-05-10 00:00:00",
         "4",
         "0.0",
         "130.0",
         "317.0",
         "429.0",
         "354.0",
         "638.0",
         "1694.0",
         "2021.0",
         "2205.0",
         "2192.0",
         "1114.0",
         "490.0",
         "Scots_pine"
        ],
        [
         "23",
         "2018-05-19 00:00:00",
         "4",
         "0.0",
         "139.0",
         "201.0",
         "311.0",
         "249.0",
         "515.0",
         "1401.0",
         "1714.0",
         "1808.0",
         "2004.0",
         "960.0",
         "441.0",
         "Scots_pine"
        ],
        [
         "24",
         "2018-06-04 00:00:00",
         "4",
         "0.0",
         "155.0",
         "198.0",
         "339.0",
         "299.0",
         "583.0",
         "1517.0",
         "1842.0",
         "2109.0",
         "2130.0",
         "976.0",
         "501.0",
         "Scots_pine"
        ],
        [
         "25",
         "2018-06-12 00:00:00",
         "4",
         "0.0",
         "163.0",
         "238.0",
         "383.0",
         "311.0",
         "647.0",
         "1616.0",
         "2055.0",
         "2080.0",
         "2346.0",
         "1063.0",
         "495.0",
         "Scots_pine"
        ],
        [
         "26",
         "2018-06-24 00:00:00",
         "4",
         "0.0",
         "175.0",
         "229.0",
         "386.0",
         "293.0",
         "650.0",
         "1609.0",
         "1985.0",
         "2050.0",
         "2280.0",
         "953.0",
         "452.0",
         "Scots_pine"
        ],
        [
         "27",
         "2018-07-11 00:00:00",
         "4",
         "0.0",
         "192.0",
         "266.0",
         "415.0",
         "309.0",
         "680.0",
         "1784.0",
         "1985.0",
         "2285.0",
         "2374.0",
         "1059.0",
         "479.0",
         "Scots_pine"
        ],
        [
         "28",
         "2018-07-13 00:00:00",
         "4",
         "0.0",
         "194.0",
         "506.0",
         "640.0",
         "562.0",
         "876.0",
         "1823.0",
         "2211.0",
         "2449.0",
         "2487.0",
         "1247.0",
         "636.0",
         "Scots_pine"
        ],
        [
         "29",
         "2018-07-17 00:00:00",
         "4",
         "0.0",
         "198.0",
         "287.0",
         "464.0",
         "355.0",
         "687.0",
         "1695.0",
         "2114.0",
         "2125.0",
         "2243.0",
         "1111.0",
         "528.0",
         "Scots_pine"
        ],
        [
         "30",
         "2018-07-24 00:00:00",
         "4",
         "0.0",
         "205.0",
         "236.0",
         "374.0",
         "325.0",
         "683.0",
         "1768.0",
         "2006.0",
         "2174.0",
         "2309.0",
         "979.0",
         "483.0",
         "Scots_pine"
        ],
        [
         "31",
         "2018-07-26 00:00:00",
         "4",
         "0.0",
         "207.0",
         "268.0",
         "407.0",
         "323.0",
         "654.0",
         "1645.0",
         "2084.0",
         "2237.0",
         "2399.0",
         "1072.0",
         "470.0",
         "Scots_pine"
        ],
        [
         "32",
         "2018-07-30 00:00:00",
         "4",
         "0.0",
         "211.0",
         "334.0",
         "508.0",
         "427.0",
         "807.0",
         "1839.0",
         "2291.0",
         "2327.0",
         "2490.0",
         "1125.0",
         "537.0",
         "Scots_pine"
        ],
        [
         "33",
         "2018-07-31 00:00:00",
         "4",
         "0.0",
         "212.0",
         "377.0",
         "534.0",
         "460.0",
         "749.0",
         "1747.0",
         "2157.0",
         "2235.0",
         "2551.0",
         "1085.0",
         "490.0",
         "Scots_pine"
        ],
        [
         "34",
         "2018-08-02 00:00:00",
         "4",
         "0.0",
         "214.0",
         "321.0",
         "460.0",
         "371.0",
         "703.0",
         "1667.0",
         "2157.0",
         "2245.0",
         "2330.0",
         "973.0",
         "442.0",
         "Scots_pine"
        ],
        [
         "35",
         "2018-08-06 00:00:00",
         "4",
         "0.0",
         "218.0",
         "450.0",
         "593.0",
         "527.0",
         "815.0",
         "1951.0",
         "2360.0",
         "2561.0",
         "2507.0",
         "1152.0",
         "487.0",
         "Scots_pine"
        ],
        [
         "36",
         "2018-08-21 00:00:00",
         "4",
         "0.0",
         "233.0",
         "434.0",
         "580.0",
         "494.0",
         "822.0",
         "1938.0",
         "2361.0",
         "2398.0",
         "2690.0",
         "1085.0",
         "489.0",
         "Scots_pine"
        ],
        [
         "37",
         "2018-09-06 00:00:00",
         "4",
         "0.0",
         "249.0",
         "348.0",
         "447.0",
         "389.0",
         "625.0",
         "1523.0",
         "1797.0",
         "1901.0",
         "1895.0",
         "779.0",
         "330.0",
         "Scots_pine"
        ],
        [
         "38",
         "2018-09-16 00:00:00",
         "4",
         "0.0",
         "259.0",
         "480.0",
         "574.0",
         "522.0",
         "819.0",
         "1907.0",
         "2111.0",
         "2241.0",
         "2510.0",
         "1047.0",
         "464.0",
         "Scots_pine"
        ],
        [
         "39",
         "2018-09-21 00:00:00",
         "4",
         "0.0",
         "264.0",
         "237.0",
         "378.0",
         "346.0",
         "604.0",
         "1610.0",
         "2026.0",
         "2232.0",
         "2401.0",
         "849.0",
         "383.0",
         "Scots_pine"
        ],
        [
         "40",
         "2018-10-09 00:00:00",
         "4",
         "0.0",
         "282.0",
         "293.0",
         "432.0",
         "365.0",
         "642.0",
         "1701.0",
         "2143.0",
         "2429.0",
         "2220.0",
         "941.0",
         "379.0",
         "Scots_pine"
        ],
        [
         "41",
         "2018-10-13 00:00:00",
         "4",
         "0.0",
         "286.0",
         "307.0",
         "473.0",
         "390.0",
         "677.0",
         "1718.0",
         "2176.0",
         "2464.0",
         "2333.0",
         "926.0",
         "434.0",
         "Scots_pine"
        ],
        [
         "42",
         "2018-10-15 00:00:00",
         "4",
         "0.0",
         "288.0",
         "424.0",
         "563.0",
         "475.0",
         "744.0",
         "1786.0",
         "2173.0",
         "2403.0",
         "2376.0",
         "1004.0",
         "455.0",
         "Scots_pine"
        ],
        [
         "43",
         "2018-11-03 00:00:00",
         "4",
         "0.0",
         "307.0",
         "298.0",
         "374.0",
         "264.0",
         "472.0",
         "1280.0",
         "1460.0",
         "1673.0",
         "1626.0",
         "571.0",
         "237.0",
         "Scots_pine"
        ],
        [
         "44",
         "2018-11-14 00:00:00",
         "4",
         "0.0",
         "318.0",
         "154.0",
         "264.0",
         "135.0",
         "384.0",
         "1201.0",
         "1621.0",
         "1668.5",
         "1778.0",
         "413.0",
         "261.0",
         "Scots_pine"
        ],
        [
         "45",
         "2018-11-15 00:00:00",
         "4",
         "0.0",
         "319.0",
         "208.0",
         "226.0",
         "137.0",
         "477.0",
         "1370.0",
         "1782.0",
         "1664.0",
         "1930.0",
         "661.0",
         "285.0",
         "Scots_pine"
        ],
        [
         "46",
         "2018-12-04 00:00:00",
         "4",
         "0.0",
         "338.0",
         "262.0",
         "289.5",
         "190.0",
         "385.0",
         "1287.0",
         "1579.5",
         "1322.0",
         "1733.0",
         "625.6666666666666",
         "270.3333333333333",
         "Scots_pine"
        ],
        [
         "47",
         "2019-01-20 00:00:00",
         "4",
         "0.0",
         "20.0",
         "316.0",
         "353.0",
         "243.0",
         "536.0",
         "1204.0",
         "1377.0",
         "1574.5",
         "1536.0",
         "590.3333333333334",
         "255.66666666666663",
         "Scots_pine"
        ],
        [
         "48",
         "2019-02-05 00:00:00",
         "4",
         "0.0",
         "36.0",
         "185.0",
         "312.0",
         "202.0",
         "488.0",
         "1476.0",
         "1510.0",
         "1827.0",
         "1731.0",
         "555.0",
         "241.0",
         "Scots_pine"
        ],
        [
         "49",
         "2019-02-13 00:00:00",
         "4",
         "0.0",
         "44.0",
         "211.0",
         "389.0",
         "234.0",
         "534.0",
         "1446.0",
         "1756.0",
         "1739.0",
         "1821.0",
         "735.0",
         "324.0",
         "Scots_pine"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 9714
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>disturbance_year</th>\n",
       "      <th>doy</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b8a</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Scots_pine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Scots_pine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-19</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>1762.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>Scots_pine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-03-27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>1776.0</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>Scots_pine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>1803.0</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>Scots_pine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>2062.5</td>\n",
       "      <td>2542.5</td>\n",
       "      <td>2341.0</td>\n",
       "      <td>2829.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>Norway_spruce_mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9710</th>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>2454.0</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2648.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>Norway_spruce_mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9711</th>\n",
       "      <td>2022-07-24</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>1818.0</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>Norway_spruce_mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9712</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>2235.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2617.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>Norway_spruce_mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>2022-10-29</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>989.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Norway_spruce_mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9714 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time  id  disturbance_year    doy     b2     b3     b4     b5  \\\n",
       "0    2017-01-13   4               0.0   13.0  176.0  229.0  205.0  405.0   \n",
       "1    2017-01-27   4               0.0   27.0  382.0  429.0  205.0  405.0   \n",
       "2    2017-02-19   4               0.0   50.0  240.0  382.0  259.0  544.0   \n",
       "3    2017-03-27   4               0.0   86.0  244.0  375.0  287.0  579.0   \n",
       "4    2017-04-27   4               0.0  117.0  215.0  327.0  274.0  548.0   \n",
       "...         ...  ..               ...    ...    ...    ...    ...    ...   \n",
       "9709 2022-06-27  99               0.0  178.0  285.0  474.0  271.0  767.0   \n",
       "9710 2022-07-23  99               0.0  204.0  235.0  403.0  247.0  627.0   \n",
       "9711 2022-07-24  99               0.0  205.0  172.0  315.0  157.0  540.0   \n",
       "9712 2022-08-01  99               0.0  213.0  240.0  417.0  244.0  638.0   \n",
       "9713 2022-10-29  99               0.0  302.0  292.0  352.0  331.0  565.0   \n",
       "\n",
       "          b6      b7      b8     b8a     b11    b12              species  \n",
       "0     1192.0  1308.0  1316.0  1529.0   396.0  180.0           Scots_pine  \n",
       "1     1192.0  1308.0  1316.0  1529.0   396.0  180.0           Scots_pine  \n",
       "2     1387.0  1609.0  1825.0  1762.0   732.0  338.0           Scots_pine  \n",
       "3     1536.0  1776.0  1878.0  1884.0  1013.0  488.0           Scots_pine  \n",
       "4     1499.0  1803.0  1766.0  1957.0  1003.0  540.0           Scots_pine  \n",
       "...      ...     ...     ...     ...     ...    ...                  ...  \n",
       "9709  2062.5  2542.5  2341.0  2829.0  1158.0  569.0  Norway_spruce_mixed  \n",
       "9710  1936.0  2454.0  2211.0  2648.0  1162.0  547.0  Norway_spruce_mixed  \n",
       "9711  1818.0  2214.0  2166.0  2620.0  1100.0  494.0  Norway_spruce_mixed  \n",
       "9712  1956.0  2235.0  2376.0  2617.0  1184.0  518.0  Norway_spruce_mixed  \n",
       "9713   989.0  1047.0  1061.0  1288.0   574.0  339.0  Norway_spruce_mixed  \n",
       "\n",
       "[9714 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df, val_df = load_data(TRAIN_PATH, TEST_PATH, VAL_PATH)\n",
    "models = define_models()\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b611f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "class EnsemblePipeline:\n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.categorical_cols = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def drop_columns(self, df):\n",
    "        \"\"\"Remove unnecessary columns.\"\"\"\n",
    "        return df.drop(\n",
    "            columns=[\"time\", \"id\", \"disturbed\", \"disturbance_year\"], errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "    def _encode_features(self, df):\n",
    "        \"\"\"One-hot encode categorical features.\"\"\"\n",
    "        return pd.get_dummies(df, columns=self.categorical_cols, drop_first=True)\n",
    "\n",
    "    def fit(self, train_df):\n",
    "        ts_builder = TimeSeriesAggregator(window=120, step=90)\n",
    "        feature_df = ts_builder.run(train_df)\n",
    "\n",
    "        feature_df[self.target_col] = (\n",
    "            train_df.groupby(\"id\")[self.target_col].first().values\n",
    "        )\n",
    "\n",
    "        df = self.drop_columns(feature_df.copy())\n",
    "\n",
    "        df[self.target_col] = self.label_encoder.fit_transform(df[self.target_col])\n",
    "\n",
    "        self.categorical_cols = [\n",
    "            c\n",
    "            for c in df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if c != self.target_col\n",
    "        ]\n",
    "\n",
    "        df = self._encode_features(df)\n",
    "        X, y = df.drop(columns=[self.target_col]), df[self.target_col]\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.fitted = True\n",
    "\n",
    "        return pd.DataFrame(X_scaled, columns=X.columns, index=X.index), y\n",
    "\n",
    "    def transform(self, df):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Pipeline must be fitted before transform().\")\n",
    "\n",
    "        ts_builder = TimeSeriesAggregator(window=120, step=90)\n",
    "        feature_df = ts_builder.run(df)\n",
    "\n",
    "        if self.target_col in df.columns:\n",
    "            feature_df[self.target_col] = (\n",
    "                df.groupby(\"id\")[self.target_col].first().values\n",
    "            )\n",
    "\n",
    "        df = self.drop_columns(feature_df.copy())\n",
    "\n",
    "        if self.target_col in df.columns:\n",
    "            df[self.target_col] = self.label_encoder.transform(df[self.target_col])\n",
    "\n",
    "        df = self._encode_features(df)\n",
    "        X = df.drop(columns=[self.target_col]) if self.target_col in df.columns else df\n",
    "        X = X.reindex(columns=self.scaler.feature_names_in_, fill_value=0)\n",
    "\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        y = df[self.target_col] if self.target_col in df.columns else None\n",
    "\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        return (X_scaled_df, y) if y is not None else X_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb2e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from models.ensemble_models.ensemble_utils.ensemble_model_manager import (\n",
    "    EnsembleModelManager,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class GenericPipeline:\n",
    "    def __init__(self, target_col=\"species\"):\n",
    "        self.pipeline = EnsemblePipeline(target_col=target_col)\n",
    "        self.ensemble = EnsembleModelManager()\n",
    "\n",
    "    def _print_metrics(self, model_name, metrics, top_n_features=5):\n",
    "        \"\"\"\n",
    "        Nicely prints the model metrics and top feature importances.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'=' * 40}\\nModel: {model_name}\\n{'=' * 40}\")\n",
    "\n",
    "        # Key performance metrics\n",
    "        metrics_table = [\n",
    "            [\"Accuracy\", metrics.get(\"accuracy\")],\n",
    "            [\"Precision (Macro)\", metrics.get(\"precision_macro\")],\n",
    "            [\"Recall (Macro)\", metrics.get(\"recall_macro\")],\n",
    "            [\"F1 (Macro)\", metrics.get(\"f1_macro\")],\n",
    "        ]\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        print(\n",
    "            tabulate(\n",
    "                metrics_table,\n",
    "                headers=[\"Metric\", \"Score\"],\n",
    "                tablefmt=\"fancy_grid\",\n",
    "                floatfmt=\".4f\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        feat_imp = metrics.get(\"feature_importances\")\n",
    "        if feat_imp:\n",
    "            feat_imp_df = (\n",
    "                pd.DataFrame(feat_imp)\n",
    "                .sort_values(by=\"importance\", ascending=False)\n",
    "                .head(top_n_features)\n",
    "            )\n",
    "            print(f\"\\nTop {top_n_features} Feature Importances:\")\n",
    "            print(\n",
    "                tabulate(\n",
    "                    feat_imp_df, headers=\"keys\", tablefmt=\"fancy_grid\", floatfmt=\".4f\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def run(self, train_df, test_df, model_defs, val_df=None):\n",
    "        \"\"\"\n",
    "        Trains and evaluates all models defined in model_defs.\n",
    "        Optionally evaluates a validation set.\n",
    "        \"\"\"\n",
    "        X_train, y_train = self.pipeline.fit(train_df)\n",
    "        X_test, y_test = self.pipeline.transform(test_df)\n",
    "        X_val, y_val = self.pipeline.transform(val_df)\n",
    "\n",
    "        feature_names = (\n",
    "            X_train.columns\n",
    "            if hasattr(X_train, \"columns\")\n",
    "            else [f\"f{i}\" for i in range(X_train.shape[1])]\n",
    "        )\n",
    "\n",
    "        results_summary = []\n",
    "\n",
    "        for model_class, params in model_defs:\n",
    "            model_name = model_class.__name__\n",
    "            print(f\"\\n{'-' * 30}\\nTraining {model_name}...\\n{'-' * 30}\")\n",
    "\n",
    "            if any(isinstance(v, (list, tuple)) for v in params.values()):\n",
    "                grid = GridSearchCV(\n",
    "                    model_class(), params, cv=5, n_jobs=-1, scoring=\"accuracy\"\n",
    "                )\n",
    "                grid.fit(X_train, y_train)\n",
    "                hyperparams = grid.best_params_\n",
    "                print(f\"Best hyperparameters: {hyperparams}\")\n",
    "            else:\n",
    "                hyperparams = params\n",
    "\n",
    "            model, train_metrics = self.ensemble.run_training(\n",
    "                model_class,\n",
    "                hyperparams,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                feature_names,\n",
    "            )\n",
    "\n",
    "            _, test_metrics = self.ensemble.run_training(\n",
    "                model_class,\n",
    "                hyperparams,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                feature_names,\n",
    "            )\n",
    "\n",
    "            _, val_metrics = self.ensemble.run_training(\n",
    "                model_class,\n",
    "                hyperparams,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_val,\n",
    "                y_val,\n",
    "                feature_names,\n",
    "            )\n",
    "\n",
    "            results_summary.append(\n",
    "                {\n",
    "                    \"model\": model_name,\n",
    "                    \"train_acc\": train_metrics[\"accuracy\"],\n",
    "                    \"test_acc\": test_metrics[\"accuracy\"],\n",
    "                    \"val_acc\": val_metrics[\"accuracy\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            self._print_metrics(model_name, test_metrics)\n",
    "        return self.ensemble.load_models(), X_train\n",
    "\n",
    "    def _plot_performance_comparison(self, results_summary):\n",
    "        df = pd.DataFrame(results_summary)\n",
    "        df.set_index(\"model\", inplace=True)\n",
    "        df.plot(kind=\"bar\", figsize=(10, 6))\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Train / Test / Validation Performance Comparison\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis=\"y\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "026ac432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 8/70 [00:00<00:07,  8.53it/s], ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ndvi'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/rafael/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'ndvi'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/rafael/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/rafael/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rafael/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2826354/3381268964.py\", line 42, in process_id\n  File \"/home/rafael/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rafael/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'ndvi'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results, x_t = \u001b[43mGenericPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspecies\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_df\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mGenericPipeline.run\u001b[39m\u001b[34m(self, train_df, test_df, model_defs, val_df)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_df, test_df, model_defs, val_df=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     52\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    Trains and evaluates all models defined in model_defs.\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m    Optionally evaluates a validation set.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     X_train, y_train = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     X_test, y_test = \u001b[38;5;28mself\u001b[39m.pipeline.transform(test_df)\n\u001b[32m     58\u001b[39m     X_val, y_val = \u001b[38;5;28mself\u001b[39m.pipeline.transform(val_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mEnsemblePipeline.fit\u001b[39m\u001b[34m(self, train_df)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_df):\n\u001b[32m     24\u001b[39m     ts_builder = TimeSeriesAggregator(window=\u001b[32m120\u001b[39m, step=\u001b[32m90\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     feature_df = \u001b[43mts_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     feature_df[\u001b[38;5;28mself\u001b[39m.target_col] = (\n\u001b[32m     28\u001b[39m         train_df.groupby(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[38;5;28mself\u001b[39m.target_col].first().values\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     31\u001b[39m     df = \u001b[38;5;28mself\u001b[39m.drop_columns(feature_df.copy())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mTimeSeriesAggregator.run\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[32m     74\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Shortcut: vollständiger Lauf.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     df_win = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggregate_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aggregate_to_single_row_keep_windows(df_win)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mTimeSeriesAggregator.aggregate_time_series\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     57\u001b[39m groups = \u001b[38;5;28mlist\u001b[39m(df.groupby(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm_joblib(tqdm(desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing IDs\u001b[39m\u001b[33m\"\u001b[39m, total=\u001b[38;5;28mlen\u001b[39m(groups))) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     res = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame([f \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m res \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m sub])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rafael_riesle/studium/semester_7/awp2/tree_classification/venv/lib/python3.12/site-packages/joblib/parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'ndvi'"
     ]
    }
   ],
   "source": [
    "results, x_t = GenericPipeline(target_col=\"species\").run(\n",
    "    train_df, test_df, models, val_df=val_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e009530",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "385fe5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b2',\n",
       " 'b3',\n",
       " 'b4',\n",
       " 'b5',\n",
       " 'b6',\n",
       " 'b7',\n",
       " 'b8',\n",
       " 'b8a',\n",
       " 'b11',\n",
       " 'b12',\n",
       " 'ndvi',\n",
       " 'gndvi',\n",
       " 'wdvi',\n",
       " 'tndvi',\n",
       " 'savi',\n",
       " 'ipvi',\n",
       " 'mcari',\n",
       " 'reip',\n",
       " 'masvi2',\n",
       " 'dvi']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_FEATURES = spectral_bands + indices\n",
    "DEFAULT_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92417e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6d390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0cb3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1aa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6192c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
