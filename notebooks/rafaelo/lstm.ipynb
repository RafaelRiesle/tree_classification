{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0cc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from your_model_file import YourLSTMModel  # deine Modellklasse importieren\n",
    "\n",
    "# Pfad zur gespeicherten Checkpoint-Datei\n",
    "ckpt_path = (\n",
    "    \"../../data/lstm_training/results/species_model-epoch=33-val_loss=0.2559.ckpt\"\n",
    ")\n",
    "\n",
    "# Modell laden\n",
    "model = YourLSTMModel.load_from_checkpoint(ckpt_path)\n",
    "model.eval()\n",
    "\n",
    "# Beispiel-Eingabe (z. B. Sequenz von 10 Schritten mit 8 Features)\n",
    "import torch\n",
    "\n",
    "x = torch.randn(1, 10, 8)  # (batch_size, seq_length, input_dim)\n",
    "\n",
    "# Vorhersage\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190befa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_metrics(self):\n",
    "    \"\"\"Berechnet verschiedene Testmetriken.\"\"\"\n",
    "    self.model.eval()\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "\n",
    "    for batch in self.data_module.test_dataloader():\n",
    "        x = batch[\"sequence\"].to(self.device)\n",
    "        y = batch[\"label\"].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(x)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())  # falls binÃ¤re Klassifikation\n",
    "\n",
    "    # In NumPy konvertieren\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Verschiedene Metriken berechnen\n",
    "    metrics = {\n",
    "        \"accuracy\": np.mean(all_preds == all_labels),\n",
    "        \"precision\": precision_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        \"recall\": recall_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        \"f1_score\": f1_score(all_labels, all_preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "    # ROC-AUC nur fÃ¼r binÃ¤re Klassifikation\n",
    "    if len(np.unique(all_labels)) == 2:\n",
    "        metrics[\"roc_auc\"] = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(\"\\nðŸ“Š Test Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:>10}: {v:.4f}\")\n",
    "\n",
    "    print(\"\\nðŸ§¾ Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator(model, data_module, feature_columns)\n",
    "metrics = evaluator.evaluate_metrics()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
